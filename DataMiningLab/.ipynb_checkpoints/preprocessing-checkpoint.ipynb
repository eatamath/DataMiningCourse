{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree.py init\n",
      "columns {'uid': 'uid', 'tran_time': 'sldatime', 'gender_age': 'cmrid', 'vipno': 'vipno', 'itemno': 'pluno', 'amount': 'amt', 'quantity': 'qty', 'brandno': 'bndno', 'class1': 'class1', 'class2': 'class2', 'class3': 'class3', 'class4': 'class4', 'class5': 'class5'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import *\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.neighbors import *\n",
    "from collections import *\n",
    "from pyclust import *\n",
    "from sklearn.manifold import *\n",
    "from sklearn.cluster import KMeans\n",
    "from module.EKmeans import *\n",
    "from module.CTree import *\n",
    "import datetime\n",
    "from collections import *\n",
    "import copy\n",
    "\n",
    "DATA_PATH = './trade_new.csv'\n",
    "\n",
    "_col = {\n",
    "    'uid':'uid',\n",
    "    'tran_time':'sldatime', #\n",
    "    'gender_age':'cmrid',\n",
    "    'vipno':'vipno', #\n",
    "    'itemno':'pluno', #\n",
    "    'amount':'amt', #\n",
    "    'quantity':'qty', #\n",
    "    'brandno':'bndno', #\n",
    "}\n",
    "\n",
    "_col_class = {\n",
    "    'class1':'class1',\n",
    "    'class2':'class2',\n",
    "    'class3':'class3',\n",
    "    'class4':'class4',\n",
    "    'class5':'class5',\n",
    "}\n",
    "\n",
    "_col2 ={**_col,**_col_class}\n",
    "\n",
    "print('columns',_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df[_col.values()]\n",
    "new_df = pd.DataFrame([],columns=_col2.values())\n",
    "\n",
    "\n",
    "category = df[_col2['itemno']].values.astype('str')\n",
    "category = np.array(list(map(lambda x:np.array([x[:2],x[:3],x[:4],x[:5],x[5:]]),category)))\n",
    "category = pd.DataFrame(category,columns=_col_class.values())\n",
    "if (category.index.start == df.index.start) and \\\n",
    "    (category.index.stop == df.index.stop):\n",
    "    new_df = df.join(category)\n",
    "\n",
    "df_amount_sum = new_df.groupby([_col2['vipno'],_col2['class1'],_col2['class2'],_col2['class3'],_col2['class4']],as_index=False).agg({\n",
    "    _col2['amount']:sum\n",
    "})\n",
    "df_amount_sum3 = new_df.groupby([_col2['vipno'],_col2['class1'],_col2['class2'],_col2['class3']],as_index=False).agg({\n",
    "    _col2['amount']:sum\n",
    "})\n",
    "df_amount_sum2 = new_df.groupby([_col2['vipno'],_col2['class1'],_col2['class2']],as_index=False).agg({\n",
    "    _col2['amount']:sum\n",
    "})\n",
    "df_amount_sum1 = new_df.groupby([_col2['vipno'],_col2['class1']],as_index=False).agg({\n",
    "    _col2['amount']:sum\n",
    "})\n",
    "df_amount = [df_amount_sum1, df_amount_sum2, df_amount_sum3, df_amount_sum]\n",
    "del df\n",
    "# df_amount_sum.to_csv('./result/a1-amount_sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "set_vipno = set(df_amount_sum[_col2['vipno']])\n",
    "mapping_index_v = {i:idx for (idx,i) in zip(range(len(set_vipno)),set_vipno)}\n",
    "\n",
    "\n",
    "def aggByUser(category='class4'):\n",
    "    global set_vipno,df_amount,_col2\n",
    "    map_single = {v:{} for v in set_vipno}\n",
    "    df_temp = df_amount[ int(category[-1])-1 ]\n",
    "    for v in set_vipno:\n",
    "        lst_v = df_temp.loc[ df_temp[_col2['vipno']]==v ]\n",
    "        for idx,row in lst_v.iterrows():\n",
    "            map_single[v][ row[ _col2[ category ] ] ] = row[ _col2['amount'] ]\n",
    "    return map_single\n",
    "\n",
    "\n",
    "def computeUserFeatureOfClass(category='class4'):\n",
    "    global df_amount,mapping_index_v,_col2\n",
    "    set_class_i = set(new_df[_col2[ category ]])\n",
    "    mapping_index_class_i = {i:idx for (idx,i) in zip(range(len(set_class_i)),set_class_i)}\n",
    "    mat_userFeature = np.array([[0.0 for i in range(len(set_class_i))] for j in range(len(set_vipno))])\n",
    "    for idx,r in df_amount[ int(category[-1])-1 ].iterrows():\n",
    "        mat_userFeature[ mapping_index_v[ r[ _col2['vipno'] ] ] ][ mapping_index_class_i[ r[ _col2[category] ] ] ] = r[ _col2['amount'] ]\n",
    "    return mat_userFeature\n",
    "\n",
    "\n",
    "def computeJaccardDistance(a,b):\n",
    "    return 1-np.sum(np.min([a,b],0))/np.sum(np.max([a,b],0))\n",
    "\n",
    "\n",
    "def computeJaccardMatrix(mat_userFeature):\n",
    "    global set_vipno,map_single,mapping_index_v,D\n",
    "    mat_jaccard = np.array([[0 for i in range(len(set_vipno))] for j in range(len(set_vipno))]).astype('float')\n",
    "    for v1 in set_vipno:\n",
    "        for v2 in set_vipno:\n",
    "            jaccard_pairwise_dist = compute_distance(a=mat_userFeature[mapping_index_v[v1]],b=mat_userFeature[mapping_index_v[v2]],D=D,metric='cos')\n",
    "            mat_jaccard[mapping_index_v[v1]][mapping_index_v[v2]] = mat_jaccard[mapping_index_v[v2]][mapping_index_v[v1]] = jaccard_pairwise_dist\n",
    "    return mat_jaccard\n",
    "\n",
    "def compute_distance(a,b,D,metric='jaccard'):\n",
    "    if metric=='jaccard':\n",
    "        return computeJaccardDistance(a,b)\n",
    "    elif metric=='euclidean':\n",
    "        return norm(a-b)\n",
    "    elif metric=='cos':\n",
    "        score = [computeJaccardDistance(a[:D[0]],b[:D[0]]),\n",
    "                computeJaccardDistance(a[D[0]:D[1]],b[D[0]:D[1]]),\n",
    "                computeJaccardDistance(a[D[1]:D[2]],b[D[1]:D[2]]),\n",
    "                computeJaccardDistance(a[D[2]:D[3]],b[D[2]:D[3]])]\n",
    "#             print(score)\n",
    "        return sum(score)/4\n",
    "\n",
    "class EKmeans():\n",
    "    def __init__(self,X,D,method='jaccard',n_centers=2,max_iters=1000,tol=1e-20,verbose=0):\n",
    "        self.n_centers = n_centers\n",
    "        self.max_iters = max_iters\n",
    "        self.D = D\n",
    "        self.method = method\n",
    "        self.X = X\n",
    "        self.clusters = np.array([-1 for i in range(X.shape[0])])\n",
    "        self.centers = self.X[np.random.permutation(X.shape[0])[:self.n_centers]]\n",
    "        self.pre_centers = []\n",
    "        self.debug = []\n",
    "        self.lr = 0.34\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        return\n",
    "    \n",
    "    def metrics_compactness(self):\n",
    "        s = 0.0\n",
    "        for c in range(self.n_centers):\n",
    "            idx_cluster = np.where(self.clusters==c)[0]\n",
    "            for a in self.X[ idx_cluster ]:\n",
    "                dst = compute_distance(a=a,b=self.centers[c],D=self.D,metric=self.method)\n",
    "                s += dst*1.0/len(idx_cluster)\n",
    "        return s\n",
    "                \n",
    "    def __update_centers(self,smooth=True):\n",
    "        self.pre_centers = np.array(self.centers)\n",
    "        for c in range(self.n_centers):\n",
    "            temp = self.X[ np.where(self.clusters==c)[0] ]\n",
    "            if len(temp):\n",
    "                self.centers[c] = np.array([ np.mean( temp, 0 ) ])\n",
    "#             print(self.centers,self.X[ np.where(self.clusters==c)[0] ]) ### debug\n",
    "        if smooth:\n",
    "            self.centers = (self.centers-self.pre_centers)*self.lr + self.pre_centers\n",
    "        return\n",
    "    \n",
    "    def __log(self,x):\n",
    "        if self.verbose:\n",
    "            print(x)\n",
    "        return\n",
    "    \n",
    "    def fit(self):\n",
    "        for kiter in range(self.max_iters):\n",
    "            self.__log('iteration %d'%kiter)\n",
    "            for i in range(self.X.shape[0]):\n",
    "                d_min_dist = 1e10\n",
    "                record_node = -1\n",
    "                for idx,c in zip(range(len(self.centers)),self.centers):\n",
    "                    dst = compute_distance(a=self.X[i],b=c,D=self.D,metric=self.method)\n",
    "                    if dst<d_min_dist:\n",
    "                        d_min_dist = dst\n",
    "                        record_node = idx\n",
    "                self.clusters[i] = record_node\n",
    "            self.__update_centers()\n",
    "            move_dist = np.mean(norm(self.pre_centers-self.centers,axis=1))\n",
    "            self.debug.append(move_dist)\n",
    "            if (kiter - kiter//20 *20 ==1) and move_dist<self.tol:\n",
    "                break\n",
    "            if kiter==10:\n",
    "#                 raise Exception('stop')\n",
    "                pass\n",
    "        return\n",
    "\n",
    "\n",
    "# map_single = aggByUser()\n",
    "mat_userFeature = [computeUserFeatureOfClass('class1'),\n",
    "                   computeUserFeatureOfClass('class2'),\n",
    "                   computeUserFeatureOfClass('class3'),\n",
    "                   computeUserFeatureOfClass('class4')\n",
    "                  ]\n",
    "\n",
    "D =np.cumsum(\n",
    "    [\n",
    "        mat_userFeature[0].shape[1],\n",
    "        mat_userFeature[1].shape[1],\n",
    "        mat_userFeature[2].shape[1],\n",
    "        mat_userFeature[3].shape[1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "mat_jaccard = computeJaccardMatrix(np.hstack(mat_userFeature))\n",
    "\n",
    "def KmeansCluster(n_centers,method='cos',max_iters=500):\n",
    "    global mat_userFeature,D\n",
    "    if method=='jaccard':\n",
    "        kmeans = EKmeans(mat_userFeature[-1],None,n_centers=n_centers,max_iters=max_iters)\n",
    "    elif method=='cos':\n",
    "        kmeans = EKmeans(np.hstack(mat_userFeature),D,n_centers=n_centers,max_iters=max_iters,method='cos')\n",
    "    elif method=='euclidean':\n",
    "        kmeans = EKmeans(mat_userFeature[-1],None,n_centers=n_centers,max_iters=max_iters,method='euclidean')\n",
    "    kmeans.fit()\n",
    "    return kmeans\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "clusters = kmeans.fit_predict(mat_userFeature[-1])\n",
    "silhouette_score(mat_userFeature[-1],labels=clusters)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for nn in range(2,20):\n",
    "    print('param ',nn)\n",
    "    kmeans = KmeansCluster(nn,'cos',1000)\n",
    "    sil = silhouette_score(mat_jaccard,labels=kmeans.clusters,metric='precomputed')\n",
    "    com = kmeans.metrics_compactness() \n",
    "    res.append({'sil':sil,'com':com,'nn':nn})\n",
    "\n",
    "# tsne = TSNE(n_jobs=-1,method='exact',n_components=2)\n",
    "# mat_userFeature_t = tsne.fit_transform(np.hstack(mat_userFeature))\n",
    "\n",
    "# sns.set(style=\"ticks\")\n",
    "# sns.pairplot(pd.DataFrame(mat_userFeature[:,100:110]),kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 将时间转化为 time level 1-4\n",
    "def processTimeLevel(new_df):\n",
    "    global _col2\n",
    "    _col2.update({\n",
    "        'tran_time_level':'tran_time_level',\n",
    "        'tran_date':'tran_date'\n",
    "    })\n",
    "    \n",
    "    tran_time = new_df[ _col2['tran_time'] ]\n",
    "    tran_time = list(map(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').timestamp(),tran_time))\n",
    "    new_df[ _col2['tran_time'] ] = tran_time\n",
    "    \n",
    "    tran_date = list( map(lambda x:datetime.datetime.strftime(datetime.datetime.fromtimestamp(x),'%Y%m%d'),tran_time) ) ### to y-m-d\n",
    "    new_df[ _col2['tran_date'] ] = tran_date\n",
    "    \n",
    "    df_temp = new_df[ [_col2['vipno'], _col2['tran_date']] ]\n",
    "#     df_temp = df_temp.groupby(by=['vipno']).agg({_col2['tran_date']:max}).reset_index().values\n",
    "#     df_temp = {x[0]:x[1] for x in df_temp}\n",
    "    q = max(df_temp[ _col2['tran_date'] ])\n",
    "    df_temp = {row['vipno']:q for idx,row in df_temp.iterrows()}\n",
    "#     print(set(new_df['vipno'])-set(df_temp.keys()))\n",
    "    tran_time_level = []\n",
    "    for idx,row in new_df.iterrows():\n",
    "        latest_date = q\n",
    "        latest_date = datetime.datetime.strptime(latest_date,'%Y%m%d').timestamp()\n",
    "        latest_date = datetime.datetime.fromtimestamp(latest_date)\n",
    "        time_level4 = (latest_date - datetime.timedelta(days=30)).timestamp()\n",
    "        time_level3 = (latest_date - datetime.timedelta(days=2*30)).timestamp()\n",
    "        time_level2 = (latest_date - datetime.timedelta(days=4*30)).timestamp()\n",
    "#         time_level1 = (latest_date - datetime.timedelta(days=16*30)).timestamp()\n",
    "        time_levels = np.array([time_level2,time_level3,time_level4])\n",
    "        row_time = row[_col2['tran_time']]\n",
    "        tran_time_level.append( len(np.where(row_time>time_levels)[0])+1 ) ### append time level\n",
    "        \n",
    "    new_df[ _col2['tran_time_level'] ] = tran_time_level\n",
    "    return new_df\n",
    "\n",
    "\n",
    "new_df = processTimeLevel(new_df)\n",
    "new_df = new_df.sort_values(by=[_col2['vipno']]) ### sort by customer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### create single tree by one transaction row\n",
    "def createTreeBySingleTrans(row):\n",
    "    troot = TreeNode(nodeType='root')\n",
    "#     root.display()\n",
    "    vipno = row[ _col2['vipno'] ]\n",
    "    class1 = row[_col2['class1']]\n",
    "    class2 = row[_col2['class2']]\n",
    "    class3 = row[_col2['class3']]\n",
    "    class4 = row[_col2['class4']]\n",
    "    tran_date = row[_col2['tran_date']]\n",
    "    newnode = TreeNode(nodeType='innode',nodeClass=class1,level=1,freq=1,tm=tran_date,parent=troot)\n",
    "    troot.att_childs.append(newnode)\n",
    "    cur_node = troot.att_childs[0]\n",
    "    for idx,c in enumerate([class2,class3,class4],2):\n",
    "#         cur_node.out()\n",
    "        if idx==4:\n",
    "            nodeType='leaf'\n",
    "            f=0\n",
    "        else:\n",
    "            if idx==row[_col2['tran_time_level']]:\n",
    "                nodeType = 'leaf'\n",
    "                f=0\n",
    "            else:\n",
    "                f=1\n",
    "                nodeType='innode'\n",
    "        newnode = TreeNode(nodeType=nodeType,nodeClass=c,level=idx,freq=1,tm=tran_date,parent=cur_node)\n",
    "        cur_node.att_childs.append(newnode)\n",
    "        cur_node = cur_node.att_childs[0]\n",
    "        if f==0:\n",
    "            break\n",
    "    return {'vipno':vipno,'root':troot}\n",
    "    \n",
    "\n",
    "def mergeNode(node1,node2,parent_node,tag='union'):\n",
    "    if node1.att_class == node2.att_class and \\\n",
    "        node1.att_level == node2.att_level:\n",
    "        if 'leaf' in [node1.node_type,node2.node_type]:\n",
    "            node_type = 'innode' if tag=='union' else 'leaf'\n",
    "        else:\n",
    "            node_type = 'innode'\n",
    "#             if node1.node_type!=node2.node_type:\n",
    "#                 raise Exception('error',node1.out(),node2.out())\n",
    "        return TreeNode(nodeType=node1.node_type,\n",
    "                        nodeClass=node1.att_class,\n",
    "                        level=node1.att_level,\n",
    "                        freq=sum([node1.att_freq,node2.att_freq]),\n",
    "                        tm=str(max(int(node1.att_tm),int(node2.att_tm))),\n",
    "                        parent=parent_node)\n",
    "    else:\n",
    "        raise Exception('error',node1.out(),node2.out())\n",
    "        return\n",
    "\n",
    "    \n",
    "def findNodeByClass(root,nodeClass,level=4):\n",
    "    res = root.get_all_nodes()\n",
    "    res = list(filter(lambda x:x.att_class==nodeClass,res)) ### find the node\n",
    "    if len(res):\n",
    "        return res[0]\n",
    "    else:\n",
    "#         print('not found class',nodeClass)\n",
    "        raise Warning('not found class')\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def unionTreeOfCusotomer2(root1,root2,vipno=False):\n",
    "    res = {}\n",
    "    root = TreeNode(nodeType='root')\n",
    "    res['tree'] = root\n",
    "    \n",
    "    if vipno:\n",
    "        res['vipno'] = vipno\n",
    "    try:\n",
    "        current_level = []\n",
    "        [current_level.extend(x.att_childs) for x in [root1,root2]]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        root1.display()\n",
    "        root2.display()\n",
    "        raise e\n",
    "        \n",
    "    next_level = []\n",
    "    for i in range(1,5):\n",
    "        level = i\n",
    "        class_counter = Counter([x.att_class for x in current_level])\n",
    "        class_intersect = list(filter(lambda x:class_counter[x]>1,class_counter))\n",
    "        class_non_intersect = list(filter(lambda x:class_counter[x]==1,class_counter))\n",
    "        while len(current_level):\n",
    "            node = current_level.pop(0)\n",
    "            if node.att_class in class_intersect:\n",
    "                parent_node = findNodeByClass(root=root,nodeClass=node.att_parent.att_class,level=node.att_parent.att_level)\n",
    "                if parent_node is None:\n",
    "                    node.out()\n",
    "                    raise Exception('no parent')\n",
    "                node2 = list(filter(lambda x:x.att_class==node.att_class,current_level))[0]\n",
    "                current_level.remove(node2)\n",
    "                parent_node.att_childs.append(mergeNode(node,node2,parent_node))\n",
    "                next_level.extend(node.att_childs)\n",
    "                next_level.extend(node2.att_childs)\n",
    "            else:\n",
    "                parent_node = findNodeByClass(root=root,nodeClass=node.att_parent.att_class,level=node.att_parent.att_level)\n",
    "                parent_node.att_childs.append(node)\n",
    "                node.att_parent = parent_node\n",
    "        current_level = next_level\n",
    "        next_level = []\n",
    "         \n",
    "    return res\n",
    "\n",
    "\n",
    "def unionTreeOfCusotomer(root,root1,root2,vipno=False):\n",
    "    q = [x for x in root1.att_childs]\n",
    "    q.extend([x for x in root2.att_childs])\n",
    "    counter = Counter(list(map(lambda x:x.att_class,q)))\n",
    "    inter = list(filter(lambda x:counter[x]>1,counter))\n",
    "    noninter = list(filter(lambda x:counter[x]==1,counter))\n",
    "    while len(q):\n",
    "        nn = q.pop(0)\n",
    "        if nn.att_class in inter:\n",
    "            nn2 = list(filter(lambda x:x.att_class==nn.att_class,q))[0]\n",
    "            q.remove(nn2)\n",
    "            newnode = mergeNode(nn,nn2,root)\n",
    "            root.att_childs.append(newnode)\n",
    "            unionTreeOfCusotomer(newnode,nn,nn2)\n",
    "        else:\n",
    "            newnode = TreeNode(nn.node_type,nn.att_class,[],nn.att_freq,nn.att_level,nn.att_tm,root)\n",
    "            nn.copy(nn,newnode)\n",
    "            root.att_childs.append(newnode)\n",
    "         \n",
    "    return \n",
    "\n",
    "def intersectTreeOfCusotomer1(root1,root2,vipno=False):\n",
    "    res = {}\n",
    "    root = TreeNode(nodeType='root')\n",
    "    res['tree'] = root\n",
    "    \n",
    "    if vipno:\n",
    "        res['vipno'] = vipno\n",
    "    \n",
    "    current_level = []\n",
    "    [current_level.extend(x.att_childs) for x in [root1,root2]]\n",
    "    next_level = []\n",
    "    for i in range(1,5):\n",
    "        level = i\n",
    "        class_counter = Counter([x.att_class for x in current_level])\n",
    "        class_intersect = list(filter(lambda x:class_counter[x]>1,class_counter))\n",
    "        class_non_intersect = list(filter(lambda x:class_counter[x]==1,class_counter))\n",
    "        while len(current_level):\n",
    "            node = current_level.pop(0)\n",
    "            if node.att_class in class_intersect:\n",
    "                parent_node = findNodeByClass(root=root,nodeClass=node.att_parent.att_class,level=node.att_parent.att_level)\n",
    "                node2 = list(filter(lambda x:x.att_class==node.att_class,current_level))[0] ### get node to be merged\n",
    "                current_level.remove(node2)\n",
    "                parent_node.att_childs.append(mergeNode(node,node2,parent_node,tag='intersect')) ### add node\n",
    "                next_level.extend(node.att_childs)\n",
    "                next_level.extend(node2.att_childs)\n",
    "        current_level = next_level\n",
    "        next_level = []\n",
    "         \n",
    "    return res\n",
    "\n",
    "def intersectTreeOfCusotomer(root,root1,root2,vipno=False):\n",
    "    q = [x for x in root1.att_childs]\n",
    "    q.extend([x for x in root2.att_childs])\n",
    "    counter = Counter(list(map(lambda x:x.att_class,q)))\n",
    "    inter = list(filter(lambda x:counter[x]>1,counter))\n",
    "    noninter = list(filter(lambda x:counter[x]==1,counter))\n",
    "    while len(q):\n",
    "        nn = q.pop(0)\n",
    "        if nn.att_class in inter:\n",
    "            nn2 = list(filter(lambda x:x.att_class==nn.att_class,q))[0]\n",
    "            q.remove(nn2)\n",
    "            newnode = mergeNode(nn,nn2,root)\n",
    "            root.att_childs.append(newnode)\n",
    "#             print('append ',newnode.att_class)\n",
    "            intersectTreeOfCusotomer(newnode,nn,nn2)\n",
    "#         else:\n",
    "#             newnode = TreeNode(nn.node_type,nn.att_class,[],nn.att_freq,nn.att_level,nn.att_tm,root)\n",
    "#             nn.copy(nn,newnode)\n",
    "#             root.att_childs.append(newnode)\n",
    "         \n",
    "    return \n",
    "\n",
    "\n",
    "def buildTreeOfCustomers(df,all_customers):\n",
    "    global _col2\n",
    "    customer_forest = {c:TreeNode(nodeType='root') for c in all_customers}\n",
    "    for idx,row in df.iterrows():\n",
    "        cust = row[ _col2['vipno'] ]\n",
    "        subtree = createTreeBySingleTrans(row)['root']\n",
    "        temp_root = TreeNode()\n",
    "        unionTreeOfCusotomer(temp_root,customer_forest[cust],subtree)\n",
    "        customer_forest[cust] = temp_root\n",
    "    return customer_forest\n",
    "\n",
    "\n",
    "def sim_v_ur(v,ur):\n",
    "    try:\n",
    "        parentnode_ur = findNodeByClass(root=ur,nodeClass=v.att_parent.att_class)\n",
    "    except Warning as e:\n",
    "        return 0\n",
    "#     if parentnode_ur is None:\n",
    "#         return 0\n",
    "    freq_ur = [x.att_freq for x in parentnode_ur.att_childs]\n",
    "    try:\n",
    "        res = float(v.att_freq/np.sum(freq_ur))\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "\n",
    "def sim_vl_ur(vl,ur):\n",
    "    if len(vl)==0:\n",
    "        return 0\n",
    "    try:\n",
    "        s = list(map(lambda x:sim_v_ur(x,ur),vl))\n",
    "        return np.mean(s)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "        \n",
    "def dist_ir_ur(ir,ur):\n",
    "    utree = TreeNode()\n",
    "    unionTreeOfCusotomer(utree,ir,ur)\n",
    "    itree = TreeNode()\n",
    "    intersectTreeOfCusotomer(itree,ir,ur)\n",
    "    ir = itree\n",
    "    ur = utree\n",
    "    nodes_ir = ir.get_all_nodes()\n",
    "    if len(nodes_ir)==0:\n",
    "        return 1\n",
    "    H = max(set(list(map(lambda x:x.att_level,nodes_ir))))\n",
    "    s = 0\n",
    "    for i in range(1,H+1):\n",
    "        w = i/np.sum(range(1,H+1))\n",
    "        vl = list(filter(lambda x:x.att_level==i,nodes_ir))\n",
    "        s += w*sim_vl_ur(vl,ur)\n",
    "    return 1-s\n",
    "\n",
    "def updateTreeByFrequence2(root,freq):\n",
    "    current_level = [x for x in root.att_childs]\n",
    "    while len(current_level):\n",
    "        x = current_level.pop(0)\n",
    "        if x.att_freq>=freq:\n",
    "            current_level.extend(x.att_childs)\n",
    "        else:\n",
    "            try:\n",
    "                p = x.att_parent\n",
    "                p.att_childs.remove(x)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return x\n",
    "\n",
    "    return root\n",
    "\n",
    "def updateTreeByFrequence(root,freq):\n",
    "    nodes = [x for x in root.att_childs]\n",
    "    for nn in nodes:\n",
    "        if nn.att_freq<freq:\n",
    "            root.att_childs.remove(nn)\n",
    "            nn.att_parent = None\n",
    "        else:\n",
    "            updateTreeByFrequence(nn,freq)\n",
    "    return\n",
    "\n",
    "def getClusterCentroid(forest_customer):\n",
    "    global customers\n",
    "    centroid = TreeNode(nodeType='root')\n",
    "    for tr in forest_customer:\n",
    "        temp_root = TreeNode()\n",
    "        unionTreeOfCusotomer(temp_root,centroid,forest_customer[tr])\n",
    "        centroid = temp_root\n",
    "    \n",
    "    current_freq = 1\n",
    "    mindist = 1e10\n",
    "    freq_step = centroid.get_avg_freq()\n",
    "    freq_end = centroid.get_max_freq()\n",
    "    centroid_record = centroid\n",
    "    size_average = np.mean(list(map(lambda x:forest_customer[x].get_node_size(),forest_customer)))\n",
    "    print('average node',size_average)\n",
    "    \n",
    "    while current_freq<freq_end:\n",
    "        updateTreeByFrequence(centroid,current_freq)\n",
    "        dist = sum([dist_ir_ur(forest_customer[c],centroid) for c in forest_customer])\n",
    "        if dist<mindist:\n",
    "            mindist = dist\n",
    "            centroid_record = TreeNode()\n",
    "            centroid.copy(centroid,centroid_record)\n",
    "        if centroid.get_node_size()<size_average:\n",
    "            break\n",
    "        current_freq += freq_step\n",
    "    return centroid_record\n",
    "\n",
    "    \n",
    "\n",
    "def clusterMain():\n",
    "    \n",
    "    ### get distant pair\n",
    "    def distantPair(topK_key,forest_concern):\n",
    "        max_dist = 0\n",
    "        max_pair = None\n",
    "        for i in topK_key:\n",
    "            for j in topK_key:\n",
    "                if j>i:\n",
    "                    dist = dist_ir_ur(forest_concern[i],forest_concern[j])\n",
    "                    if dist>max_dist:\n",
    "                        max_dist = dist\n",
    "                        max_pair = (i,j)\n",
    "        return [max_dist,max_pair]\n",
    "\n",
    "    ### recluster forest_concern\n",
    "    def recluster():\n",
    "        subcluster = [[],[]]\n",
    "        for c in forest_concern:\n",
    "            current_tree = forest_concern[c]\n",
    "            [dist1,dist2] = [\n",
    "                dist_ir_ur(current_tree,forest_concern[max_pair[0]]),\n",
    "                dist_ir_ur(current_tree,forest_concern[max_pair[1]])\n",
    "            ]\n",
    "            if dist1<dist2:\n",
    "                subcluster[0].append(c)\n",
    "            else:\n",
    "                subcluster[1].append(c)\n",
    "        return subcluster\n",
    "\n",
    "    ### precluster [] ; postcluster [[]]\n",
    "    def computeBIC(precluster,postcluster):\n",
    "\n",
    "        N = len(precluster)\n",
    "        D = sum([customer_buy_count[x] for x in precluster])\n",
    "        pre_coff_1 = (N-1)\n",
    "        pre_sigma_s = 1.0/pre_coff_1 * np.sum(np.power(list(dist_concern.values()),2))\n",
    "        pre_likelihood = -N/2*np.log(2*np.pi) - N*D/2*np.log(pre_sigma_s) - pre_coff_1/2\n",
    "        pre_bic = pre_likelihood - 1*(D+1)/2*np.log(1)\n",
    "\n",
    "        N_i = np.array([len(postcluster[0]), len(postcluster[1])])\n",
    "        D_i = np.array([\n",
    "            sum([customer_buy_count[x] for x in postcluster[0]]),\n",
    "            sum([customer_buy_count[x] for x in postcluster[1]])\n",
    "        ])\n",
    "        post_coff_1 = N_i - 2\n",
    "        post_sigma_s = 1.0/post_coff_1 * np.array([ \n",
    "            np.sum( np.power(list( dist_1.values() ),2) ), \n",
    "            np.sum( np.power(list( dist_2.values() ),2) )\n",
    "        ])\n",
    "        post_likelihood = N_i*np.log(N_i) - N_i*np.log(N) - N_i/2*np.log(2*np.pi) - N_i*D/2*np.log(post_sigma_s) - post_coff_1/2\n",
    "        post_likelihood = np.sum(post_likelihood)\n",
    "        post_bic = post_likelihood - 2*(D+1)/2*np.log(2)\n",
    "\n",
    "        print('bic ',pre_bic,post_bic)\n",
    "        return pre_bic,post_bic\n",
    "    \n",
    "    ### compute metrics and scores\n",
    "    def computeMetrics(cluster_final):\n",
    "    #     cluster_final = cluster_static\n",
    "        mapping = {x:i for i,x in enumerate(customers,0)} ### customer:idx\n",
    "        d = np.array([[0.0 for i in range(len(customers))] for j in range(len(customers))])\n",
    "        ### 计算距离矩阵\n",
    "        for i,x in enumerate(customers,0):\n",
    "            for j,y in enumerate(customers,0):\n",
    "                if j>=i:\n",
    "                    d[ mapping[x] ][ mapping[y] ] = dist_ir_ur(forest_customer[x],forest_customer[y])\n",
    "                    d[ mapping[y] ][ mapping[x] ] = d[ mapping[x] ][ mapping[y] ]\n",
    "\n",
    "        sc = 0\n",
    "        B = np.array([0.0 for i in range(len(customers))])\n",
    "        A = np.array([0.0 for j in range(len(customers))])\n",
    "        cust_all = set(customers)\n",
    "        for m,cl in enumerate(cluster_final,0):\n",
    "            cust_cl = set(cl)\n",
    "            for p in cl:\n",
    "                same = cust_cl - set([p])\n",
    "                #### find nearest neighbor centroid\n",
    "                record_nearest = None\n",
    "                record_mindist = 100\n",
    "                for n,ct in enumerate(centroid_final,0):\n",
    "                    if m!=n:\n",
    "                        temp_dist = dist_ir_ur(forest_customer[p],ct)\n",
    "                        if temp_dist<record_mindist:\n",
    "                            record_mindist = temp_dist\n",
    "                            record_nearest = n\n",
    "\n",
    "                other = cluster_final[n]\n",
    "                A[ mapping[p] ] = np.mean([d[ mapping[p] ][ mapping[s] ] for s in same])\n",
    "                B[ mapping[p] ] = np.mean([d[ mapping[p] ][ mapping[o] ] for o in other])\n",
    "        sc = np.mean([(B[i]-A[i])/np.max([A[i],B[i]]) for i in range(len(customers))])\n",
    "        print('sc ',sc)\n",
    "\n",
    "        cp = np.mean([ \n",
    "            np.mean( list(map(lambda x:dist_ir_ur(forest_customer[x],centroid_final[i]),cl)) ) \n",
    "            for i,cl in enumerate(cluster_final,0) \n",
    "        ])\n",
    "        print('cp ',cp)\n",
    "\n",
    "        return sc,cp\n",
    "\n",
    "    top_ratio = 0.5\n",
    "    topK_max = 50\n",
    "    split_threshold = 20\n",
    "    global new_df\n",
    "    customer_buy_count = new_df.groupby(by=[_col2['vipno']]).count()['uid'].to_dict() ### { customer:buy_count }\n",
    "    customers = list( set(new_df[_col2['vipno']]) ) ### all customers : list\n",
    "    forest_customer = buildTreeOfCustomers(new_df,customers) ### customer tree {vipno:root}\n",
    "    cluster_dynamic = [[x for x in customers]]\n",
    "    cluster_static = []\n",
    "    centroid_final = []\n",
    "    while len(cluster_dynamic):\n",
    "        cluster_concern = cluster_dynamic.pop(0)\n",
    "        cluster_concern = set(cluster_concern)\n",
    "        forest_concern = list(filter(lambda x:x in cluster_concern,forest_customer))\n",
    "        forest_concern = { x:forest_customer[x] for x in forest_concern }\n",
    "        centroid_concern = getClusterCentroid(forest_concern) ### centoid tree of interest\n",
    "        dist_concern = { x:dist_ir_ur(forest_concern[x],centroid_concern) for x in forest_concern } ### dist of @forest_concern & @centroid_concern\n",
    "        topK = min( [ topK_max,max([2,top_ratio*len(forest_concern)]) ] )\n",
    "        topK = int(topK)\n",
    "        topK_key = sorted(dist_concern,key=lambda x:dist_concern[x],reverse=True)[:topK]\n",
    "        [max_dist,max_pair] = distantPair(topK_key,forest_concern) ### find most distant pair\n",
    "        subcluster = recluster()\n",
    "        subforest_1 = { x:forest_customer[x] for x in subcluster[0] }\n",
    "        subforest_2 = { x:forest_customer[x] for x in subcluster[1] }\n",
    "        centroid_1 = getClusterCentroid(subforest_1) ### costing\n",
    "        centroid_2 = getClusterCentroid(subforest_2) ### costing\n",
    "        dist_1 = { x:dist_ir_ur(subforest_1[x],centroid_1) for x in subcluster[0] } ### costing\n",
    "        dist_2 = { x:dist_ir_ur(subforest_2[x],centroid_2) for x in subcluster[1] } ### costing\n",
    "        [ pre_bic,post_bic ] = computeBIC(list(cluster_concern),subcluster)\n",
    "        if post_bic<pre_bic or (len(subforest_1)<split_threshold or len(subforest_2)<split_threshold):\n",
    "            cluster_static.append(list(cluster_concern))\n",
    "            centroid_final.append(centroid_concern)\n",
    "            print('settle ',len(cluster_concern))\n",
    "        else:\n",
    "            cluster_dynamic.extend(subcluster)\n",
    "            print('split ',len(subcluster[0]),len(subcluster[1]))\n",
    "    return computeMetrics(cluster_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### get distant pair\n",
    "def distantPair(topK_key,forest_concern):\n",
    "    max_dist = 0\n",
    "    max_pair = None\n",
    "    for i in topK_key:\n",
    "        for j in topK_key:\n",
    "            if j>i:\n",
    "                dist = dist_ir_ur(forest_concern[i],forest_concern[j])\n",
    "                if dist>max_dist:\n",
    "                    max_dist = dist\n",
    "                    max_pair = (i,j)\n",
    "    return [max_dist,max_pair]\n",
    "\n",
    "### recluster forest_concern\n",
    "def recluster():\n",
    "    subcluster = [[],[]]\n",
    "    for c in forest_concern:\n",
    "        current_tree = forest_concern[c]\n",
    "        [dist1,dist2] = [\n",
    "            dist_ir_ur(current_tree,forest_concern[max_pair[0]]),\n",
    "            dist_ir_ur(current_tree,forest_concern[max_pair[1]])\n",
    "        ]\n",
    "        if dist1<dist2:\n",
    "            subcluster[0].append(c)\n",
    "        else:\n",
    "            subcluster[1].append(c)\n",
    "    return subcluster\n",
    "\n",
    "### precluster [] ; postcluster [[]]\n",
    "def computeBIC(precluster,postcluster):\n",
    "\n",
    "    N = len(precluster)\n",
    "    D = sum([customer_buy_count[x] for x in precluster])\n",
    "    pre_coff_1 = (N-1)\n",
    "    pre_sigma_s = 1.0/pre_coff_1 * np.sum(np.power(list(dist_concern.values()),2))\n",
    "    pre_likelihood = -N/2*np.log(2*np.pi) - N*D/2*np.log(pre_sigma_s) - pre_coff_1/2\n",
    "    pre_bic = pre_likelihood - 1*(D+1)/2*np.log(1)\n",
    "\n",
    "    N_i = np.array([len(postcluster[0]), len(postcluster[1])])\n",
    "    D_i = np.array([\n",
    "        sum([customer_buy_count[x] for x in postcluster[0]]),\n",
    "        sum([customer_buy_count[x] for x in postcluster[1]])\n",
    "    ])\n",
    "    post_coff_1 = N_i - 2\n",
    "    post_sigma_s = 1.0/post_coff_1 * np.array([ \n",
    "        np.sum( np.power(list( dist_1.values() ),2) ), \n",
    "        np.sum( np.power(list( dist_2.values() ),2) )\n",
    "    ])\n",
    "    post_likelihood = N_i*np.log(N_i) - N_i*np.log(N) - N_i/2*np.log(2*np.pi) - N_i*D/2*np.log(post_sigma_s) - post_coff_1/2\n",
    "    post_likelihood = np.sum(post_likelihood)\n",
    "    post_bic = post_likelihood - 2*(D+1)/2*np.log(2)\n",
    "\n",
    "    print('bic ',pre_bic,post_bic)\n",
    "    return pre_bic,post_bic\n",
    "\n",
    "### compute metrics and scores\n",
    "def computeMetrics(cluster_final):\n",
    "#     cluster_final = cluster_static\n",
    "    mapping = {x:i for i,x in enumerate(customers,0)} ### customer:idx\n",
    "    d = np.array([[0.0 for i in range(len(customers))] for j in range(len(customers))])\n",
    "    ### 计算距离矩阵\n",
    "    for i,x in enumerate(customers,0):\n",
    "        for j,y in enumerate(customers,0):\n",
    "            if j>=i:\n",
    "                d[ mapping[x] ][ mapping[y] ] = dist_ir_ur(forest_customer[x],forest_customer[y])\n",
    "                d[ mapping[y] ][ mapping[x] ] = d[ mapping[x] ][ mapping[y] ]\n",
    "\n",
    "    sc = 0\n",
    "    B = np.array([0.0 for i in range(len(customers))])\n",
    "    A = np.array([0.0 for j in range(len(customers))])\n",
    "    cust_all = set(customers)\n",
    "    for m,cl in enumerate(cluster_final,0):\n",
    "        cust_cl = set(cl)\n",
    "        for p in cl:\n",
    "            same = cust_cl - set([p])\n",
    "            #### find nearest neighbor centroid\n",
    "            record_nearest = None\n",
    "            record_mindist = 100\n",
    "            for n,ct in enumerate(centroid_final,0):\n",
    "                if m!=n:\n",
    "                    temp_dist = dist_ir_ur(forest_customer[p],ct)\n",
    "                    if temp_dist<record_mindist:\n",
    "                        record_mindist = temp_dist\n",
    "                        record_nearest = n\n",
    "\n",
    "            other = cluster_final[n]\n",
    "            A[ mapping[p] ] = np.mean([d[ mapping[p] ][ mapping[s] ] for s in same])\n",
    "            B[ mapping[p] ] = np.mean([d[ mapping[p] ][ mapping[o] ] for o in other])\n",
    "    sc = np.mean([(B[i]-A[i])/np.max([A[i],B[i]]) for i in range(len(customers))])\n",
    "    print('sc ',sc)\n",
    "\n",
    "    cp = np.mean([ \n",
    "        np.mean( list(map(lambda x:dist_ir_ur(forest_customer[x],centroid_final[i]),cl)) ) \n",
    "        for i,cl in enumerate(cluster_final,0) \n",
    "    ])\n",
    "    print('cp ',cp)\n",
    "\n",
    "    return sc,cp\n",
    "\n",
    "top_ratio = 0.2\n",
    "topK_max = 100\n",
    "split_threshold = 10\n",
    "global new_df\n",
    "customer_buy_count = new_df.groupby(by=[_col2['vipno']]).count()['uid'].to_dict() ### { customer:buy_count }\n",
    "customers = list( set(new_df[_col2['vipno']]) ) ### all customers : list\n",
    "forest_customer = buildTreeOfCustomers(new_df,customers) ### customer tree {vipno:root}\n",
    "cluster_dynamic = [[x for x in customers]]\n",
    "cluster_static = []\n",
    "centroid_final = []\n",
    "while len(cluster_dynamic):\n",
    "    cluster_concern = cluster_dynamic.pop(0)\n",
    "    cluster_concern = set(cluster_concern)\n",
    "    forest_concern = list(filter(lambda x:x in cluster_concern,forest_customer))\n",
    "    forest_concern = { x:forest_customer[x] for x in forest_concern }\n",
    "    centroid_concern = getClusterCentroid(forest_concern) ### centoid tree of interest\n",
    "    dist_concern = { x:dist_ir_ur(forest_concern[x],centroid_concern) for x in forest_concern } ### dist of @forest_concern & @centroid_concern\n",
    "    topK = min( [ topK_max,max([2,top_ratio*len(forest_concern)]) ] )\n",
    "    topK = int(topK)\n",
    "    topK_key = sorted(dist_concern,key=lambda x:dist_concern[x],reverse=True)[:topK]\n",
    "    [max_dist,max_pair] = distantPair(topK_key,forest_concern) ### find most distant pair\n",
    "    subcluster = recluster()\n",
    "    subforest_1 = { x:forest_customer[x] for x in subcluster[0] }\n",
    "    subforest_2 = { x:forest_customer[x] for x in subcluster[1] }\n",
    "    centroid_1 = getClusterCentroid(subforest_1) ### costing\n",
    "    centroid_2 = getClusterCentroid(subforest_2) ### costing\n",
    "    dist_1 = { x:dist_ir_ur(subforest_1[x],centroid_1) for x in subcluster[0] } ### costing\n",
    "    dist_2 = { x:dist_ir_ur(subforest_2[x],centroid_2) for x in subcluster[1] } ### costing\n",
    "    [ pre_bic,post_bic ] = computeBIC(list(cluster_concern),subcluster)\n",
    "    if post_bic<pre_bic or (len(subforest_1)<split_threshold or len(subforest_2)<split_threshold):\n",
    "        cluster_static.append(list(cluster_concern))\n",
    "        centroid_final.append(centroid_concern)\n",
    "        print('settle ',len(cluster_concern))\n",
    "    else:\n",
    "        cluster_dynamic.extend(subcluster)\n",
    "        print('split ',len(subcluster[0]),len(subcluster[1]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "sc  0.1073706269222801\n",
      "cp  0.41024161544962334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1073706269222801, 0.41024161544962334)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cluster_static))\n",
    "computeMetrics(cluster_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = TreeNode()\n",
    "intersectTreeOfCusotomer(c,r1,t1)\n",
    "c.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.display()\n",
    "updateTreeByFrequence2(a,2)\n",
    "# findNodeByClass(root=a,nodeClass='',level=0)\n",
    "a.display()\n",
    "\n",
    "# b.att_childs[0].att_parent,b\n",
    "\n",
    "# .att_childs.remove(b.att_childs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### test case 1\n",
    "r1 = TreeNode(nodeType='root')\n",
    "r11=TreeNode(nodeType='innode',nodeClass='Food',freq=2,level=1,parent=r1)\n",
    "r1.att_childs.append(r11)\n",
    "r12=TreeNode(nodeType='innode',nodeClass='Clothes',freq=1,level=1,parent=r1)\n",
    "r1.att_childs.append(r12)\n",
    "\n",
    "r21=TreeNode(nodeType='innode',nodeClass='Drink',freq=2,level=2,parent=r11)\n",
    "r11.att_childs.append(r21)\n",
    "r22=TreeNode(nodeType='innode',nodeClass='Children Clothes',freq=1,level=2,parent=r12)\n",
    "r12.att_childs.append(r22)\n",
    "\n",
    "r31=TreeNode(nodeType='innode',nodeClass='Tea',freq=2,level=3,parent=r21)\n",
    "r21.att_childs.append(r31)\n",
    "\n",
    "r41=TreeNode(nodeType='innode',nodeClass='Black Tea',freq=1,level=4,parent=r31)\n",
    "r31.att_childs.append(r41)\n",
    "r42=TreeNode(nodeType='innode',nodeClass='Green Tea',freq=1,level=4,parent=r31)\n",
    "r31.att_childs.append(r42)\n",
    "\n",
    "\n",
    "\n",
    "t1 = TreeNode(nodeType='root')\n",
    "t11=TreeNode(nodeType='innode',nodeClass='Food',freq=2,level=1,parent=t1)\n",
    "t1.att_childs.append(t11)\n",
    "t12=TreeNode(nodeType='innode',nodeClass='Electronics',freq=1,level=1,parent=t1)\n",
    "t1.att_childs.append(t12)\n",
    "\n",
    "t21=TreeNode(nodeType='innode',nodeClass='Drink',freq=2,level=2,parent=t11)\n",
    "t11.att_childs.append(t21)\n",
    "\n",
    "t31=TreeNode(nodeType='innode',nodeClass='Tea',freq=1,level=3,parent=t21)\n",
    "t21.att_childs.append(t31)\n",
    "t32=TreeNode(nodeType='innode',nodeClass='Juice',freq=1,level=3,parent=t21)\n",
    "t21.att_childs.append(t32)\n",
    "\n",
    "t41=TreeNode(nodeType='innode',nodeClass='Black Tea',freq=1,level=4,parent=r31)\n",
    "t31.att_childs.append(t41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# c = TreeNode('class')\n",
    "# a.copy(a,c)\n",
    "r1.display()\n",
    "t1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = unionTreeOfCusotomer(r1,t1)['tree']\n",
    "print()\n",
    "b = intersectTreeOfCusotomer(r1,t1)['tree']\n",
    "dist_ir_ur(r1,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = new_df.iloc[0]\n",
    "createTreeBySingleTrans(item)['root'].display()\n",
    "new_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(dist_concern.values())\n",
    "r = sorted(r)\n",
    "sns.distplot(r,bins=200)\n",
    "r1 = list(dist_1.values())\n",
    "r1 = sorted(r1)\n",
    "r2 = list(dist_2.values())\n",
    "r2 = sorted(r2)\n",
    "sns.distplot(r1,bins=100)\n",
    "sns.distplot(r2,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
